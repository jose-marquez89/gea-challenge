{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model: Aggregated Probability\n",
    "After trying several approaches in the sandbox notebooks, I settled on \n",
    "the following approach given a time constraint: The aggregated probabilities\n",
    "of a nearest neighbors classifier on sequence permutations (TFIDF scores) and \n",
    "and a random forest classifier on the one-hot encoded portion of the training \n",
    "data. The following is a distillation of the best scoring process in the sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import permutations\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing The Data\n",
    "Before being able to fit any models, some feature engineering needs to be performed on the training observations, and the training labels need to be collapsed down to a usable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pd.read_csv(\"train_values.csv\")\n",
    "labels = pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>lab_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9ZIMC</td>\n",
       "      <td>RYUA3GVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5SAQC</td>\n",
       "      <td>RYUA3GVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E7QRO</td>\n",
       "      <td>RYUA3GVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT5FP</td>\n",
       "      <td>RYUA3GVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7PTD8</td>\n",
       "      <td>RYUA3GVO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sequence_id    lab_id\n",
       "0       9ZIMC  RYUA3GVO\n",
       "1       5SAQC  RYUA3GVO\n",
       "2       E7QRO  RYUA3GVO\n",
       "3       CT5FP  RYUA3GVO\n",
       "4       7PTD8  RYUA3GVO"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the main label series\n",
    "\n",
    "lab_ids = labels.columns[1:]\n",
    "\n",
    "# get numpy matrix of lab_id one-hot values\n",
    "lab_matrix = labels.drop(columns=['sequence_id']).values\n",
    "\n",
    "# get array of indices to map back to lab_ids\n",
    "lab_col_indices = np.asarray(lab_matrix == 1).nonzero()[1]\n",
    "\n",
    "labels['lab_id'] = lab_ids[lab_col_indices]\n",
    "y = labels[['sequence_id', 'lab_id']]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that all labels are correct\n",
    "def correct_label(row_n):\n",
    "    return labels.iloc[row_n][labels['lab_id'].iloc[row_n]] == 1\n",
    "\n",
    "assert all(list(map(correct_label, range(labels.shape[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to effectively use RandomizedSearchCV, some of the observations where classes had very few members needed to be \n",
    "oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2DYAZID    4\n",
      "03GRNN7N    4\n",
      "RZPGGEG4    4\n",
      "68OY1RK5    4\n",
      "VDSDXJ71    4\n",
      "1KZHNVYR    4\n",
      "UMOD7PGG    4\n",
      "PXT3AJ7C    4\n",
      "8N5EPD5C    4\n",
      "WM3Q8LBC    4\n",
      "INDCDVP0    4\n",
      "XCWSW5T9    4\n",
      "YCD71LRY    4\n",
      "LGTP4O86    4\n",
      "G2P73NZ0    3\n",
      "58BSUZQB    3\n",
      "WB78G3XF    2\n",
      "0L3Y6ZB2    1\n",
      "ON9AXMKF    1\n",
      "Name: lab_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# these are the classes with less than 5 members\n",
    "# need all to have at least 5 for randomized search cv\n",
    "print(y['lab_id'].value_counts().tail(19))\n",
    "\n",
    "# create resampling map\n",
    "resample_map = y['lab_id'].value_counts().tail(19)\n",
    "resample_map = list(zip(resample_map.index, resample_map.values))\n",
    "\n",
    "# temporarily add 'lab_id' back to\n",
    "over_samp = pd.concat([values, y['lab_id']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(df, resamp_map, min_samps=5):\n",
    "    \"\"\"\n",
    "    Randomly oversample rows that belong to classes with less than N\n",
    "    members\n",
    "    \n",
    "    df: pandas DataFrame object\n",
    "    resamp_map: reference values for classes and resample amount\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    for class_, members in resamp_map:\n",
    "        for i in range(min_samps - members):\n",
    "            new_row = new_df.loc[new_df['lab_id'] == class_].sample(n=1)\n",
    "            new_df = new_df.append(new_row)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each class has a minimum of 5 members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A4BM0B6A    5\n",
       "4RHLX089    5\n",
       "5Z4CMIY5    5\n",
       "V4A28VLV    5\n",
       "QNKGHIRB    5\n",
       "YCD71LRY    5\n",
       "MH0GC0GY    5\n",
       "LUHRMKEB    5\n",
       "2L336TQL    5\n",
       "58BSUZQB    5\n",
       "SBQXQOPV    5\n",
       "RZPGGEG4    5\n",
       "1KZHNVYR    5\n",
       "TU2W2LCB    5\n",
       "L905DK46    5\n",
       "UYLJZRPN    5\n",
       "DJW5U56I    5\n",
       "WM3Q8LBC    5\n",
       "0L3Y6ZB2    5\n",
       "W2DYAZID    5\n",
       "Name: lab_id, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_samp = oversample(over_samp, resample_map)\n",
    "over_samp['lab_id'].value_counts().tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is oversampled for use with RandomizedSearchCV, a function can be created that will take train and test data, and output the appropriate data set for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNGT', 'CNGA', 'CNTG', 'CNTA', 'CNAG']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, ngrams for sequences must be made\n",
    "N_GRAMS = 4\n",
    "\n",
    "# create the 'vocabulary' for the different nucleotides\n",
    "n_tides = set(''.join(over_samp['sequence'].values))\n",
    "\n",
    "# create a list of subsequences for features\n",
    "subseqs = list(''.join(p) for p in permutations(n_tides, r=N_GRAMS))\n",
    "subseqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(df, ss, transform=False, normalizer=None):\n",
    "    \"\"\"\n",
    "    Splits train or testing data into one-hot\n",
    "    encoded data and tokenized sequences\n",
    "    \n",
    "    df: The pandas dataframe to be split\n",
    "    \n",
    "    ss: The subsequences to tokenize from\n",
    "    \n",
    "    transform: whether to transform \n",
    "               tokens as opposed to fitting\n",
    "               \n",
    "    score_func: function to apply to token counts\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    one_hot: pandas DataFrame object with one-hot columns\n",
    "    \n",
    "    sub_sequences: pandas DataFrame object with subsequences vectorized\n",
    "    \"\"\"\n",
    "    sub_sequences = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for seq in ss:\n",
    "        sub_sequences[seq] = df[\"sequence\"].str.count(seq)\n",
    "    \n",
    "    one_hot = df.drop(columns=[\"sequence\", \"sequence_id\"])\n",
    "    \n",
    "    if not transform:\n",
    "        # call fit_transform on normalizer\n",
    "        sub_sequences = normalizer.fit_transform(sub_sequences)\n",
    "    else:\n",
    "        sub_sequences = normalizer.transform(sub_sequences)\n",
    "        \n",
    "    return one_hot, sub_sequences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63046, 39)\n",
      "(63046, 120)\n",
      "(63046,)\n"
     ]
    }
   ],
   "source": [
    "# create transformer and label encoder\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "le = LabelEncoder()\n",
    "\n",
    "# set y first and encode\n",
    "y = over_samp['lab_id']\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "over_samp = over_samp.drop(columns=['lab_id'])\n",
    "\n",
    "# create one-hot X and sequence X\n",
    "Xoh, Xsq = separate(over_samp, subseqs, normalizer=tfidf)\n",
    "\n",
    "print(Xoh.shape)\n",
    "print(Xsq.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Models\n",
    "The most successful models were produced by a cross-validated randomized search through a nearest neighbors classifier and a \n",
    "random forest classifier. These are two separate searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  4.0min finished\n"
     ]
    }
   ],
   "source": [
    "# contruct rfc with random search\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "\n",
    "params = {\"n_estimators\": range(50, 150, 25),\n",
    "          \"criterion\": [\"gini\", \"entropy\"],\n",
    "          \"max_depth\": range(5, 20, 5),\n",
    "          \"min_samples_leaf\": range(1, 10),\n",
    "          \"max_features\": [\"sqrt\", \"log2\"]}\n",
    "\n",
    "rs = RandomizedSearchCV(rfc, params, random_state=0, verbose=1, n_jobs=-1, n_iter=5)\n",
    "rfc_search = rs.fit(Xoh, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed: 14.6min finished\n"
     ]
    }
   ],
   "source": [
    "# construct the neighbors model\n",
    "nn_classifier = KNeighborsClassifier(algorithm=\"brute\", metric=\"cosine\", leaf_size=40)\n",
    "\n",
    "params = {\"n_neighbors\": range(1,40),\n",
    "          \"weights\": [\"uniform\", \"distance\"],\n",
    "          \"leaf_size\": range(20, 50, 10)}\n",
    "\n",
    "rs_nn = RandomizedSearchCV(nn_classifier, params, random_state=0, \n",
    "                         verbose=1, n_jobs=-1, n_iter=5)\n",
    "nn_search = rs_nn.fit(Xsq, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=15, max_features='sqrt', min_samples_leaf=3,\n",
      "                       random_state=0)\n",
      "==================================================\n",
      "KNeighborsClassifier(algorithm='brute', metric='cosine', n_neighbors=12,\n",
      "                     weights='distance')\n"
     ]
    }
   ],
   "source": [
    "# best estimators\n",
    "print(rfc_search.best_estimator_)\n",
    "print(\"=\" * 50)\n",
    "print(nn_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Submission Data\n",
    "At this point, the new models need to be applied to the \n",
    "test data. This will allow the creation of an aggregated \n",
    "probability submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18816, 39)\n",
      "(18816, 120)\n"
     ]
    }
   ],
   "source": [
    "# format testing data\n",
    "test = pd.read_csv(\"test_values.csv\")\n",
    "\n",
    "# split data\n",
    "Xoh_test, Xsq_test = separate(test, subseqs, \n",
    "                              transform=True, normalizer=tfidf)\n",
    "print(Xoh_test.shape)\n",
    "print(Xsq_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for each test set\n",
    "rfc_probs = rfc_search.best_estimator_.predict_proba(Xoh_test)\n",
    "nn_probs = nn_search.best_estimator_.predict_proba(Xsq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregated probabilities\n",
    "agg_probs = (rfc_probs + nn_probs) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare submission\n",
    "sub_format = pd.read_csv(\"submission_format_3TFRxH6.csv\", index_col='sequence_id')\n",
    "\n",
    "assert sub_format.shape == agg_probs.shape\n",
    "assert (le.classes_ == sub_format.columns).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_submission = pd.DataFrame(data=agg_probs,\n",
    "                              columns=le.classes_,\n",
    "                              index=sub_format.index)\n",
    "agg_submission.to_csv(\"aggregated_submission_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-gov)",
   "language": "python",
   "name": "data-gov"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
